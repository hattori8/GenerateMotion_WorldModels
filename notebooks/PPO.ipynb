{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SH5JCHGfPHDC"
   },
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート．\n",
    "from abc import ABC, abstractmethod\n",
    "import os\n",
    "import glob\n",
    "from collections import deque\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "from base64 import b64encode\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Gymの警告を一部無視する．\n",
    "gym.logger.set_level(40)\n",
    "# matplotlibをColab上で描画するためのコマンド．\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrB7DGnRfEbx",
    "outputId": "d6d008de-b2c0-4621-9a4f-f1c061b5641d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pybullet in /usr/local/lib/python3.7/dist-packages (3.2.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipを用いてPyBulletをインストール．\n",
    "!pip install pybullet\n",
    "import pybullet_envs\n",
    "import pybullet as p\n",
    "p.connect(p.DIRECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eiTg1rJZOdkW",
    "outputId": "d903d05b-b9f5-40a2-e19d-0521b90abd89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 16 13:40:28 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   39C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWjeKg-gp4jv",
    "outputId": "e84e0ecf-60ba-4612-f4ab-5db9b325f7c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "4UVOYlrpR1Lp",
    "outputId": "6f6e2df0-8681-4f0e-de0e-c2c5ff39654c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1.10.0+cu111'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkgUuloVKlDq"
   },
   "outputs": [],
   "source": [
    "def wrap_monitor(env):\n",
    "    \"\"\" Gymの環境をmp4に保存するために，環境をラップする関数． \"\"\"\n",
    "    return gym.wrappers.Monitor(env, '/tmp/monitor', video_callable=lambda x: True, force=True)\n",
    "\n",
    "def play_mp4():\n",
    "    \"\"\" 保存したmp4をHTMLに埋め込み再生する関数． \"\"\"\n",
    "    path = glob.glob(os.path.join('/tmp/monitor', '*.mp4'))[0]\n",
    "    mp4 = open(path, 'rb').read()\n",
    "    url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    return HTML(\"\"\"<video width=400 controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "hWhImjlyTCs9",
    "outputId": "25afe41a-f7a5-4197-bb51-2c2172b59586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space:  Box(-inf, inf, (26,), float32)\n",
      "action space:  Box(-1.0, 1.0, (6,), float32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video width=400 controls><source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAAABtZGF0\" type=\"video/mp4\"></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = wrap_monitor(gym.make('HalfCheetahBulletEnv-v0'))\n",
    "\n",
    "print('observation space: ', env.observation_space)\n",
    "print('action space: ', env.action_space)\n",
    "\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "# 終了シグナル(done=True)が返ってくるまで，ランダムに環境を動かす．\n",
    "while (not done):\n",
    "  action = env.action_space.sample()\n",
    "  _, _, done, _ = env.step(np.zeros_like(action))\n",
    "\n",
    "del env\n",
    "play_mp4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2csWXzTcsgET"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, env, env_test, algo, seed=0, num_steps=10**6, eval_interval=10**4, num_eval_episodes=3):\n",
    "\n",
    "        self.env = env\n",
    "        self.env_test = env_test\n",
    "        self.algo = algo\n",
    "\n",
    "        # 環境の乱数シードを設定する．\n",
    "        self.env.seed(seed)\n",
    "        self.env_test.seed(2**31-seed)\n",
    "\n",
    "        # 平均収益を保存するための辞書．\n",
    "        self.returns = {'step': [], 'return': []}\n",
    "\n",
    "        # データ収集を行うステップ数．\n",
    "        self.num_steps = num_steps\n",
    "        # 評価の間のステップ数(インターバル)．\n",
    "        self.eval_interval = eval_interval\n",
    "        # 評価を行うエピソード数．\n",
    "        self.num_eval_episodes = num_eval_episodes\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" num_stepsステップの間，データ収集・学習・評価を繰り返す． \"\"\"\n",
    "\n",
    "        # 学習開始の時間\n",
    "        self.start_time = time()\n",
    "        # エピソードのステップ数．\n",
    "        t = 0\n",
    "\n",
    "        # 環境を初期化する．\n",
    "        state = self.env.reset()\n",
    "\n",
    "        for steps in range(1, self.num_steps + 1):\n",
    "            # 環境(self.env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            # アルゴリズムに渡し，状態・エピソードのステップ数を更新する．\n",
    "            state, t = self.algo.step(self.env, state, t, steps)\n",
    "\n",
    "            # アルゴリズムが準備できていれば，1回学習を行う．\n",
    "            if self.algo.is_update(steps):\n",
    "                self.algo.update()\n",
    "\n",
    "            # 一定のインターバルで評価する．\n",
    "            if steps % self.eval_interval == 0:\n",
    "                self.evaluate(steps)\n",
    "\n",
    "    def evaluate(self, steps):\n",
    "        \"\"\" 複数エピソード環境を動かし，平均収益を記録する． \"\"\"\n",
    "\n",
    "        returns = []\n",
    "        for _ in range(self.num_eval_episodes):\n",
    "            state = self.env_test.reset()\n",
    "            done = False\n",
    "            episode_return = 0.0\n",
    "\n",
    "            while (not done):\n",
    "                action = self.algo.exploit(state)\n",
    "                state, reward, done, _ = self.env_test.step(action)\n",
    "                episode_return += reward\n",
    "\n",
    "            returns.append(episode_return)\n",
    "\n",
    "        mean_return = np.mean(returns)\n",
    "        self.returns['step'].append(steps)\n",
    "        self.returns['return'].append(mean_return)\n",
    "\n",
    "        print(f'Num steps: {steps:<6}   '\n",
    "              f'Return: {mean_return:<5.1f}   '\n",
    "              f'Time: {self.time}')\n",
    "\n",
    "    def visualize(self):\n",
    "        \"\"\" 1エピソード環境を動かし，mp4を再生する． \"\"\"\n",
    "        env = wrap_monitor(gym.make(self.env.unwrapped.spec.id))\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while (not done):\n",
    "            action = self.algo.exploit(state)\n",
    "            state, _, done, _ = env.step(action)\n",
    "\n",
    "        del env\n",
    "        return play_mp4()\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\" 平均収益のグラフを描画する． \"\"\"\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        plt.plot(self.returns['step'], self.returns['return'])\n",
    "        plt.xlabel('Steps', fontsize=24)\n",
    "        plt.ylabel('Return', fontsize=24)\n",
    "        plt.tick_params(labelsize=18)\n",
    "        plt.title(f'{self.env.unwrapped.spec.id}', fontsize=24)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    @property\n",
    "    def time(self):\n",
    "        \"\"\" 学習開始からの経過時間． \"\"\"\n",
    "        return str(timedelta(seconds=int(time() - self.start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UngEASpOE__V"
   },
   "outputs": [],
   "source": [
    "class Algorithm(ABC):\n",
    "\n",
    "    def explore(self, state):\n",
    "        \"\"\" 確率論的な行動と，その行動の確率密度の対数 \\log(\\pi(a|s)) を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze_(0)\n",
    "        with torch.no_grad():\n",
    "            action, log_pi = self.actor.sample(state)\n",
    "        return action.cpu().numpy()[0], log_pi.item()\n",
    "\n",
    "    def exploit(self, state):\n",
    "        \"\"\" 決定論的な行動を返す． \"\"\"\n",
    "        state = torch.tensor(state, dtype=torch.float, device=self.device).unsqueeze_(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state)\n",
    "        return action.cpu().numpy()[0]\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_update(self, steps):\n",
    "        \"\"\" 現在のトータルのステップ数(steps)を受け取り，アルゴリズムを学習するか否かを返す． \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, env, state, t, steps):\n",
    "        \"\"\" 環境(env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\n",
    "            受け取り，リプレイバッファへの保存などの処理を行い，状態・エピソードのステップ数を更新する．\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update(self):\n",
    "        \"\"\" 1回分の学習を行う． \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6tuB-Ed4ULy"
   },
   "source": [
    "## 3.Proximal Policy Optimization(PPO)の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBjDdY1sACOO"
   },
   "source": [
    "#### 3.1 [演習] 方策計算のための関数の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByR90JN2ATX5"
   },
   "source": [
    "今回の演習では **共分散行列が対角なガウス分布に $\\tanh$ を適用した確率分布** をPPO・SACの方策として用います．以下では，共分散行列が対角なガウス分布を単にガウス分布と呼び，共分散行列の対角成分の平方根を単に標準偏差と呼ぶことにします．また，括弧で囲んだ表記 `(...)` は，TensorのSizeを表すことにします．\n",
    "\n",
    "方策は，学習時に探索を行うための**確率論的な行動選択**と，評価時に最適な行動を行うための**決定論的な行動選択**の2種類の行動選択を行います．確率論的な行動選択ではガウス分布からのサンプルに $\\tanh$ を適用したものを，決定論的な行動選択ではガウス分布の最頻値(平均)に $\\tanh$ を適用したものを行動とします．\n",
    "\n",
    "まず，確率論的な行動を計算した際の行動の確率密度の対数 $\\log \\pi(a|s)$ を求める関数 `calculate_log_pi(log_stds, noises, actions)` を実装しましょう．ただし，引数の `log_stds` (標準偏差の対数)，`noises` (Reparametrization Trickにおける標準ガウス分布からのノイズ)，`actions` (行動)はすべて `(batch_size, |A|)` とし，行動の確率密度の対数は `(batch_size, 1)` で返します．\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osqFBGB9muqU"
   },
   "source": [
    "(ヒント) Reparameterization Trickでは，標準ガウス分布からのノイズ $\\epsilon \\sim \\mathcal N(0, I)$ を用いて，平均 $\\mu$，標準偏差 $\\sigma$ からのサンプルを以下のように計算します．\n",
    "\n",
    "$$\n",
    "u = \\mu + \\epsilon * \\sigma\n",
    "$$\n",
    "\n",
    "確率密度関数は平行移動に関して不変なので，ガウス分布からのサンプル $u$ の確率密度 $p(u|s)$ は $\\mathcal N(0, \\sigma I)$ における $\\epsilon * \\sigma$ の確率密度として計算することができます．その後，$\\tanh$ による確率密度の変化を以下のように修正してあげましょう．\n",
    "\n",
    "$$\n",
    "\\log\\pi(a|s) = \\log p(u|s) - \\sum_{i=1}^{|\\mathcal A|} \\log (1 - \\tanh^{2}(u_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nFzrtnyABdk"
   },
   "outputs": [],
   "source": [
    "def calculate_log_pi(log_stds, noises, actions):\n",
    "    \"\"\" 確率論的な行動の確率密度を返す． \"\"\"\n",
    "    # ガウス分布 `N(0, stds * I)` における `noises * stds` の確率密度の対数(= \\log \\pi(u|a))を計算する．\n",
    "    # (torch.distributions.Normalを使うと無駄な計算が生じるので，下記では直接計算しています．)\n",
    "    gaussian_log_probs = \\\n",
    "        (-0.5 * noises.pow(2) - log_stds).sum(dim=-1, keepdim=True) - 0.5 * math.log(2 * math.pi) * log_stds.size(-1)\n",
    "\n",
    "    # tanh による確率密度の変化を修正する．\n",
    "    log_pis = gaussian_log_probs - torch.log(1 - actions.pow(2) + 1e-6).sum(dim=-1, keepdim=True)\n",
    "\n",
    "    return log_pis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrmimsobpLHq"
   },
   "source": [
    "次に，Reparameterization Trickを用いて，確率的な行動 $a = \\tanh(\\mu + \\epsilon * \\sigma)$ とその行動の確率密度の対数 $\\log \\pi(a|s)$ を計算する関数 `reparameterize(means, log_stds)` を実装しましょう．ただし，引数の `means` (平均)と `log_stds` (標準偏差の対数)は `(batch_size, |A|)` とし，行動は `(batch_size, |A|)`，確率密度の対数は `(batch_size, 1)` で返します．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXB4oSOypODW"
   },
   "outputs": [],
   "source": [
    "def reparameterize(means, log_stds):\n",
    "    \"\"\" Reparameterization Trickを用いて，確率論的な行動とその確率密度を返す． \"\"\"\n",
    "    # 標準偏差．\n",
    "    stds = log_stds.exp()\n",
    "    # 標準ガウス分布から，ノイズをサンプリングする．\n",
    "    noises = torch.randn_like(means)\n",
    "    # Reparameterization Trickを用いて，N(means, stds)からのサンプルを計算する．\n",
    "    us = means + noises * stds\n",
    "    # tanh　を適用し，確率論的な行動を計算する．\n",
    "    actions = torch.tanh(us)\n",
    "\n",
    "    # 確率論的な行動の確率密度の対数を計算する．\n",
    "    log_pis = calculate_log_pi(log_stds, noises, actions)\n",
    "\n",
    "    return actions, log_pis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFHSQdjHBDNC"
   },
   "source": [
    "また，のちのち簡単に方策を実装できるように，ある平均・標準偏差の対数でパラメータ化したガウス分布 + $\\tanh$ の方策における，ある行動の確率密度の対数を計算する関数 `evaluate_lop_pi(means, log_stds, actions)` をあらかじめ定義しておきます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMabtapvCs2I"
   },
   "outputs": [],
   "source": [
    "def atanh(x):\n",
    "    \"\"\" tanh の逆関数． \"\"\"\n",
    "    return 0.5 * (torch.log(1 + x + 1e-6) - torch.log(1 - x + 1e-6))\n",
    "\n",
    "\n",
    "def evaluate_lop_pi(means, log_stds, actions):\n",
    "    \"\"\" 平均(mean)，標準偏差の対数(log_stds)でパラメータ化した方策における，行動(actions)の確率密度の対数を計算する． \"\"\"\n",
    "    noises = (atanh(actions) - means) / (log_stds.exp() + 1e-8)\n",
    "    return calculate_log_pi(log_stds, noises, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aDzmgu88eg6"
   },
   "source": [
    "### 3.2 ネットワークの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBN4Kke02q8V"
   },
   "source": [
    "PPOの方策では，ユニット数64の隠れ層を2層もち，中間層の活性化関数に $\\tanh$ を用いたネットワークを構築します．このネットワークは，入力として状態を受け取り，**ガウス分布の平均**を出力します．また**ガウス分布の標準偏差の対数**を，学習するパラメータとして保持します．\n",
    "\n",
    "以下の4つのメソッドを持つ，PPOの方策を関数近似するネットワークのクラス `PPOActor` を実装します．\n",
    "\n",
    "- `__init__(self, state_shape, action_shape)`\n",
    "\n",
    "> 入力として状態を受け取り，**ガウス分布の平均**を出力するネットワークを構築します．また，**ガウス分布の標準偏差の対数**を表すパラメータを作成します．\n",
    "\n",
    "- `forward(self, states)`\n",
    "\n",
    "> `(batch_size, |S|)` の `states` を受け取り，決定論な行動 $a$ を `(batch_size, 1)`で返します．\n",
    "\n",
    "- `sample(self, states)`\n",
    "\n",
    "> `(batch_size, |S|)` の `states` を受け取り，確率論的な行動 $a$ とその行動の確率密度の対数 $\\log(\\pi(a|s))$ をそれぞれ `(batch_size, 1)` で返します．\n",
    "\n",
    "- `evaluate_log_pi(self, states, actions)`\n",
    "\n",
    "> `(batch_size, |S|)` の `states` と，`(batch_size, |A|)` の `actions` を受け取り，現在の方策における行動 `actions` の確率密度の対数を `(batch_size, 1)` で返します．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhNUtiaA8le7"
   },
   "outputs": [],
   "source": [
    "class PPOActor(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_shape[0], 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_shape[0]),\n",
    "        )\n",
    "        self.log_stds = nn.Parameter(torch.zeros(1, action_shape[0]))\n",
    "\n",
    "    def forward(self, states):\n",
    "        return torch.tanh(self.net(states))\n",
    "\n",
    "    def sample(self, states):\n",
    "        return reparameterize(self.net(states), self.log_stds)\n",
    "\n",
    "    def evaluate_log_pi(self, states, actions):\n",
    "        return evaluate_lop_pi(self.net(states), self.log_stds, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNC4aOG68end"
   },
   "source": [
    "続いて，PPOの状態価値を関数近似するネットワークのクラス `PPOCritic` を実装します．このネットワークも，ユニット数64の隠れ層を2層もち，中間層の活性化関数に $\\tanh$ を用います．入力として状態を受け取り，状態価値を出力します．\n",
    "\n",
    "- `__init__(self, state_shape)`\n",
    "\n",
    "> 入力として状態受け取り，状態価値を出力するネットワークを構築します．\n",
    "\n",
    "- `forward(self, states)`\n",
    "\n",
    "> `(batch_size, |S|)` の `states` を受け取り，状態価値を `(batch_size, 1)` で返します．\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "is0PicZs-D3a"
   },
   "outputs": [],
   "source": [
    "class PPOCritic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_shape[0], 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, states):\n",
    "        return self.net(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gwH2XTaVotk"
   },
   "source": [
    "### 3.3 Generalized Advantage Estimation(GAE)の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmBWfpMuVtUo"
   },
   "source": [
    "PPOでは，アドバンテージの推定に**Generalized Advantage Estimation**(GAE)[[4]](#scrollTo=HOq7n-OJboPr)を用います．GAEではnステップのアドバンテージ $\\hat A_t^{(n)} = r_t + \\gamma r_{t+1} + \\cdots + \\gamma^n V(s_{t+n}) - V(s_t)$ を用いて，以下のようにアドバンテージを推定します．\n",
    "\n",
    "$$\n",
    "\\hat A_t^{GAE}(\\lambda) = (1-\\lambda)(\\hat A_t^{(1)} + \\lambda \\hat A_t^{(2)} + \\lambda^2 \\hat A_t^{(3)} + \\cdots)\n",
    "$$\n",
    "\n",
    "このとき，TD誤差 $\\delta_t= r_t+ \\gamma V(s_{t+1}) - V(s_t)$ を用いると，以下のように式変形できます．(発展課題で導出に挑戦してみましょう！)\n",
    "\n",
    "$$\n",
    "\\hat A_t^{GAE}(\\lambda) = \\sum_{i=0}^{\\infty} (\\gamma \\lambda)^i \\delta_{t+i}\n",
    "$$\n",
    "\n",
    "従って，GAEは再帰的に計算することが可能です．ただし，実際にはロールアウト長を $T$ としたときに，$t = T+1$ 以降のGAEをすべて $0$ と近似して計算を行います．\n",
    "\n",
    "$$\n",
    "\\hat A_t^{GAE}(\\lambda) = \\delta_t + (\\gamma \\lambda) \\hat A_{t+1}^{GAE}(\\lambda)\n",
    "$$\n",
    "\n",
    "\\\\\n",
    "またPPOでは，状態価値のターゲットを $\\lambda$-収益 $R_t(\\lambda)$ を用いて推定します．\n",
    "\n",
    "$$\n",
    "R_t(\\lambda) = \\hat A_t^{GAE}(\\lambda) + V(s_t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvbFfmsbVsoS"
   },
   "outputs": [],
   "source": [
    "def calculate_advantage(values, rewards, dones, next_values, gamma=0.995, lambd=0.997):\n",
    "    \"\"\" GAEを用いて，状態価値のターゲットとGAEを計算する． \"\"\"\n",
    "\n",
    "    # TD誤差を計算する．\n",
    "    deltas = rewards + gamma * next_values * (1 - dones) - values\n",
    "\n",
    "    # GAEを初期化する．\n",
    "    advantages = torch.empty_like(rewards)\n",
    "\n",
    "    # 終端ステップを計算する．\n",
    "    advantages[-1] = deltas[-1]\n",
    "\n",
    "    # 終端ステップの1つ前から，順番にGAEを計算していく．\n",
    "    for t in reversed(range(rewards.size(0) - 1)):\n",
    "        advantages[t] = deltas[t] + gamma * lambd * (1 - dones[t]) * advantages[t + 1]\n",
    "\n",
    "    # 状態価値のターゲットをλ-収益として計算する．\n",
    "    targets = advantages + values\n",
    "\n",
    "    # GAEを標準化する．\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    return targets, advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ag6zR7a8eaE"
   },
   "source": [
    "### 3.4 [演習] 学習アルゴリズム(PPO)の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9ZL471Cvmjb"
   },
   "source": [
    "まず，収集したデータを保存するためのバッファを用意します．ここでは，時刻tの状態・行動・即時報酬・終了シグナル・確率密度の対数，そして時刻t+1の状態をロールアウト1回分保存することとします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1TyJ7mm_Btu"
   },
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "\n",
    "    def __init__(self, buffer_size, state_shape, action_shape, device=torch.device('cuda')):\n",
    "\n",
    "        # GPU上に保存するデータ．\n",
    "        self.states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "        self.actions = torch.empty((buffer_size, *action_shape), dtype=torch.float, device=device)\n",
    "        self.rewards = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.dones = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.log_pis = torch.empty((buffer_size, 1), dtype=torch.float, device=device)\n",
    "        self.next_states = torch.empty((buffer_size, *state_shape), dtype=torch.float, device=device)\n",
    "\n",
    "        # 次にデータを挿入するインデックス．\n",
    "        self._p = 0\n",
    "        # バッファのサイズ．\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def append(self, state, action, reward, done, log_pi, next_state):\n",
    "        self.states[self._p].copy_(torch.from_numpy(state))\n",
    "        self.actions[self._p].copy_(torch.from_numpy(action))\n",
    "        self.rewards[self._p] = float(reward)\n",
    "        self.dones[self._p] = float(done)\n",
    "        self.log_pis[self._p] = float(log_pi)\n",
    "        self.next_states[self._p].copy_(torch.from_numpy(next_state))\n",
    "        self._p = (self._p + 1) % self.buffer_size\n",
    "    \n",
    "    def get(self):\n",
    "        assert self._p == 0, 'Buffer needs to be full before training.'\n",
    "        return self.states, self.actions, self.rewards, self.dones, self.log_pis, self.next_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXPiwvrewX3_"
   },
   "source": [
    "では，いよいよPPOの学習アルゴリズムを実装していきましょう！\n",
    "\n",
    "今回の演習では，`update_critic` と `update_actor` の2つのメソッドを実装します．以下では，方策のネットワークのパラメータを $\\phi$，状態価値のネットワークのパラメータを $\\theta$ とします．また，ロールアウト長を $T$ とします．\n",
    "\n",
    "- `update_critic(self, states, targets)`\n",
    "\n",
    "> `(batch_size, |S|)` の `states` (状態)と `(batch_size, 1)` の `targets` (状態価値のターゲット)を受け取り，Criticのネットワークを更新します．\n",
    "\n",
    "> 状態価値のネットワークの損失関数は **平均二乗誤差** を用います．\n",
    "\n",
    "$$\n",
    "\\mathcal L^{PPO}_V(\\theta) = E_{t \\in [1, T]}[ (V_\\theta(s_t) - R_t(\\lambda))^2 ]\n",
    "$$\n",
    "\n",
    "- `update_actor(self, states, actions, log_pis_old, advantages)`\n",
    "\n",
    "> `(batch_size, |S|)` の `states` (状態)と `(batch_size, |A|)` の `actions` (行動)，そして `(batch_size, 1)` の `log_pis_old` (データ収集時の方策における行動の確率密度)と `advantages` (GAE)を受け取り，Actorのネットワークを更新します．\n",
    "\n",
    "> 方策のネットワークの損失関数は，以下の式を用います．ただし，過去の方策のネットワークのパラメータを $\\phi_{old}$ とします．また，損失関数の $\\epsilon$ は `self.clip_eps` に保持されています．\n",
    "\n",
    "$$\n",
    "\\mathcal L^{PPO}_\\pi(\\phi) = E_{t \\in [1, T]}[ \\min(\\frac{\\pi_\\phi(a_t|s_t)}{\\pi_{\\phi_{old}}(a_t|s_t)}\\hat A_t^{GAE}(\\lambda), \\; clip(\\frac{\\pi_\\phi(a_t|s_t)}{\\pi_{\\phi_{old}}(a_t|s_t)}, 1-\\epsilon, 1+\\epsilon) \\hat A_t^{GAE}(\\lambda) ]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tF2Z46rT-LND"
   },
   "outputs": [],
   "source": [
    "class PPO(Algorithm):\n",
    "\n",
    "    def __init__(self, state_shape, action_shape, device=torch.device('cuda'), seed=0,\n",
    "                 batch_size=64, gamma=0.995, lr_actor=3e-4, lr_critic=3e-4,\n",
    "                 rollout_length=2048, num_updates=10, clip_eps=0.2, lambd=0.97,\n",
    "                 coef_ent=0.0, max_grad_norm=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # シードを設定する．\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "        # データ保存用のバッファ．\n",
    "        self.buffer = RolloutBuffer(\n",
    "            buffer_size=rollout_length,\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Actor-Criticのネットワークを構築する．\n",
    "        self.actor = PPOActor(\n",
    "            state_shape=state_shape,\n",
    "            action_shape=action_shape,\n",
    "        ).to(device)\n",
    "        self.critic = PPOCritic(\n",
    "            state_shape=state_shape,\n",
    "        ).to(device)\n",
    "\n",
    "        # オプティマイザ．\n",
    "        self.optim_actor = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.optim_critic = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "\n",
    "        # その他パラメータ．\n",
    "        self.learning_steps = 0\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.rollout_length = rollout_length\n",
    "        self.num_updates = num_updates\n",
    "        self.clip_eps = clip_eps\n",
    "        self.lambd = lambd\n",
    "        self.coef_ent = coef_ent\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def is_update(self, steps):\n",
    "        # ロールアウト1回分のデータが溜まったら学習する．\n",
    "        return steps % self.rollout_length == 0\n",
    "\n",
    "    def step(self, env, state, t, steps):\n",
    "        t += 1\n",
    "\n",
    "        action, log_pi = self.explore(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # ゲームオーバーではなく，最大ステップ数に到達したことでエピソードが終了した場合は，\n",
    "        # 本来であればその先も試行が継続するはず．よって，終了シグナルをFalseにする．\n",
    "        # NOTE: ゲームオーバーによってエピソード終了した場合には， done_masked=True が適切．\n",
    "        # しかし，以下の実装では，\"たまたま\"最大ステップ数でゲームオーバーとなった場合には，\n",
    "        # done_masked=False になってしまう．\n",
    "        # その場合は稀で，多くの実装ではその誤差を無視しているので，今回も無視する．\n",
    "        if t == env._max_episode_steps:\n",
    "            done_masked = False\n",
    "        else:\n",
    "            done_masked = done\n",
    "\n",
    "        # バッファにデータを追加する．\n",
    "        self.buffer.append(state, action, reward, done_masked, log_pi, next_state)\n",
    "\n",
    "        # エピソードが終了した場合には，環境をリセットする．\n",
    "        if done:\n",
    "            t = 0\n",
    "            next_state = env.reset()\n",
    "\n",
    "        return next_state, t\n",
    "\n",
    "    def update(self):\n",
    "        self.learning_steps += 1\n",
    "\n",
    "        states, actions, rewards, dones, log_pis, next_states = self.buffer.get()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            values = self.critic(states)\n",
    "            next_values = self.critic(next_states)\n",
    "        targets, advantages = calculate_advantage(values, rewards, dones, next_values, self.gamma, self.lambd)\n",
    "\n",
    "        # バッファ内のデータを num_updates回ずつ使って，ネットワークを更新する．\n",
    "        for _ in range(self.num_updates):\n",
    "            # インデックスをシャッフルする．\n",
    "            indices = np.arange(self.rollout_length)\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            # ミニバッチに分けて学習する．\n",
    "            for start in range(0, self.rollout_length, self.batch_size):\n",
    "                idxes = indices[start:start+self.batch_size]\n",
    "                self.update_critic(states[idxes], targets[idxes])\n",
    "                self.update_actor(states[idxes], actions[idxes], log_pis[idxes], advantages[idxes])\n",
    "\n",
    "    def update_critic(self, states, targets):\n",
    "        loss_critic = (self.critic(states) - targets).pow_(2).mean()\n",
    "\n",
    "        self.optim_critic.zero_grad()\n",
    "        loss_critic.backward(retain_graph=False)\n",
    "        # 学習を安定させるヒューリスティックとして，勾配のノルムをクリッピングする．\n",
    "        nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
    "        self.optim_critic.step()\n",
    "\n",
    "    def update_actor(self, states, actions, log_pis_old, advantages):\n",
    "        log_pis = self.actor.evaluate_log_pi(states, actions)\n",
    "        mean_entropy = -log_pis.mean()\n",
    "\n",
    "        ratios = (log_pis - log_pis_old).exp_()\n",
    "        loss_actor1 = -ratios * advantages\n",
    "        loss_actor2 = -torch.clamp(\n",
    "            ratios,\n",
    "            1.0 - self.clip_eps,\n",
    "            1.0 + self.clip_eps\n",
    "        ) * advantages\n",
    "        loss_actor = torch.max(loss_actor1, loss_actor2).mean() - self.coef_ent * mean_entropy\n",
    "\n",
    "        self.optim_actor.zero_grad()\n",
    "        loss_actor.backward(retain_graph=False)\n",
    "        # 学習を安定させるヒューリスティックとして，勾配のノルムをクリッピングする．\n",
    "        nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
    "        self.optim_actor.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6A_Vf5SBHgU"
   },
   "source": [
    "### 3.5 実験"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKHWuYCV1WYI"
   },
   "source": [
    "それでは，実装したPPOを学習させてみましょう！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-B0hc3Q3KqAs"
   },
   "source": [
    "#### InvertedPendulumBulletEnv-v0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIJrWfugiC9v"
   },
   "source": [
    "まず，`InvertedPendulumBulletEnv-v0` でPPOを $3 \\times 10^4$ ステップ学習させてみましょう！学習には2~3分ほどかかります．うまく学習できると，平均収益が1000に達します．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gSeJjZxKxlP"
   },
   "source": [
    "#### HalfCheetahBulletEnv-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W59WC6JEhdin"
   },
   "source": [
    "次に，`HalfCheetahBulletEnv-v0` でPPOを $3 * 10^5$ ステップ学習させてみましょう！うまく学習できると，1500~1700程度の平均収益に達します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsiXxhuaJQns"
   },
   "outputs": [],
   "source": [
    "ENV_ID = 'HalfCheetahBulletEnv-v0'\n",
    "SEED = 0\n",
    "NUM_STEPS = 3 * 10 ** 5\n",
    "EVAL_INTERVAL = 10 ** 4\n",
    "\n",
    "env = gym.make(ENV_ID)\n",
    "env_test = gym.make(ENV_ID)\n",
    "\n",
    "algo = PPO(\n",
    "    state_shape=env.observation_space.shape,\n",
    "    action_shape=env.action_space.shape,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    env=env,\n",
    "    env_test=env_test,\n",
    "    algo=algo,\n",
    "    seed=SEED,\n",
    "    num_steps=NUM_STEPS,\n",
    "    eval_interval=EVAL_INTERVAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "6LeEeC5yVWCm",
    "outputId": "b952031e-657c-493b-b5ee-ff48ca498d3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num steps: 10000    Return: -1303.4   Time: 0:00:42\n",
      "Num steps: 20000    Return: -1322.8   Time: 0:01:28\n",
      "Num steps: 30000    Return: -1284.8   Time: 0:02:13\n",
      "Num steps: 40000    Return: 147.0   Time: 0:02:59\n",
      "Num steps: 50000    Return: -957.1   Time: 0:03:45\n",
      "Num steps: 60000    Return: -1235.2   Time: 0:04:31\n",
      "Num steps: 70000    Return: 575.6   Time: 0:05:16\n",
      "Num steps: 80000    Return: -595.8   Time: 0:06:01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-337a54701ab5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# 環境(self.env)，現在の状態(state)，現在のエピソードのステップ数(t)，今までのトータルのステップ数(steps)を\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# アルゴリズムに渡し，状態・エピソードのステップ数を更新する．\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# アルゴリズムが準備できていれば，1回学習を行う．\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-36992641d8f9>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, env, state, t, steps)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# ゲームオーバーではなく，最大ステップ数に到達したことでエピソードが終了した場合は，\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybullet_envs/gym_locomotion_envs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     ):  # TODO: Maybe calculating feet contacts could be done within the robot code\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0mcontact_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontact_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0;31m#print(\"CONTACT OF '%d' WITH %d\" % (contact_ids, \",\".join(contact_names)) )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mground_ids\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mcontact_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pybullet_envs/robot_bases.py\u001b[0m in \u001b[0;36mcontact_list\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcontact_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetContactPoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbodies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbodyIndex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbodyPartIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqYEdhFRy-NS"
   },
   "outputs": [],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBvQo9UxJQG3"
   },
   "outputs": [],
   "source": [
    "trainer.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBnQ1nR6Ms46"
   },
   "outputs": [],
   "source": [
    "del env\n",
    "del env_test\n",
    "del algo\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOq7n-OJboPr"
   },
   "source": [
    "\n",
    "\n",
    "参考文献\n",
    "\n",
    "[[1]](https://arxiv.org/abs/1707.06347) Schulman, John, et al. \"Proximal policy optimization algorithms.\" arXiv preprint arXiv:1707.06347 (2017).\n",
    "\n",
    "[[2]](https://arxiv.org/abs/1801.01290) Haarnoja, Tuomas, et al. \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.\" arXiv preprint arXiv:1801.01290 (2018).\n",
    "\n",
    "[[3]](https://arxiv.org/abs/1812.05905) Haarnoja, Tuomas, et al. \"Soft actor-critic algorithms and applications.\" arXiv preprint arXiv:1812.05905 (2018).\n",
    "\n",
    "[[4]](https://arxiv.org/abs/1506.02438) Schulman, John, et al. \"High-dimensional continuous control using generalized advantage estimation.\" arXiv preprint arXiv:1506.02438 (2015).\n",
    "\n",
    "[[5]](https://arxiv.org/abs/2006.05990) Andrychowicz, Marcin, et al. \"What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study.\" arXiv preprint arXiv:2006.05990 (2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "chap02_answer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
